{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics covered:\n",
    "- Tokenization\n",
    "    - word\n",
    "    - sentence\n",
    "- Text Normalization\n",
    "    - stemming\n",
    "    - lemmatization\n",
    "- StopWords\n",
    "- Name Entity Recognition\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization refers to break down the text into smaller units. It entails splitting paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP pipeline.\n",
    "\n",
    "These tokens can be Words or Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Terminology\n",
    "- Corpus : A large collection of sentences. (plural: Corpora)\n",
    "- Token : The smallest unit in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')  # For tokenization\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The quick brown fox jumps over the lazy dog. \n",
    "Natural Language Processing allows machines to understand human language. \n",
    "Isn't that amazing? Nope We must try hard\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox jumps over the lazy dog.', 'Natural Language Processing allows machines to understand human language.', \"Isn't that amazing?\", 'Nope We must try hard']\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "Natural Language Processing allows machines to understand human language.\n",
      "Isn't that amazing?\n",
      "Nope We must try hard\n"
     ]
    }
   ],
   "source": [
    "sentences=sent_tokenize(text)\n",
    "print(sentences)\n",
    "for i in sentences:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'Natural', 'Language', 'Processing', 'allows', 'machines', 'to', 'understand', 'human', 'language', '.', 'Is', \"n't\", 'that', 'amazing', '?', 'Nope', 'We', 'must', 'try', 'hard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ì Fun Fact\n",
    "\n",
    "## Wordnet\n",
    "WordNet is a lexical database of semantic relations between words that links words into semantic relations including synonyms, hyponyms, and meronyms.\n",
    "***\n",
    "- Your Own Personal Dictionary. But A SMART one.\n",
    "- It let you know meaning of the word, synonymns, antonymns etc.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon Example using WordNet\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "syns = wordnet.synsets(\"learning\")\n",
    "print(\"\\nLexicon Entry for 'learning':\")\n",
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.synonyms(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its root or base form. \n",
    "It may not result in an actual valid word.\n",
    "\n",
    "Example: \"running\", \"runs\", and \"ran\" may all be reduced to \"run\" or \"run\" could become \"runn\" using certain stemmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Lemmatization is also the process of reducing a word to its base form (lemma).\n",
    "However, it always returns a valid word, using a dictionary and part-of-speech (POS) tagging.\n",
    "Example: \"running\" becomes \"run\", \"better\" becomes \"good\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then What's The difference? ü§î\n",
    "1. Stemming is faster but less accurate.\n",
    "2. Lemmatization is slower but more accurate and meaningful.\n",
    "3. Stemming may produce non-existent words; Lemmatization always returns real words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer    # stemming\n",
    "from nltk.stem import WordNetLemmatizer   #lemmatizing\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText=\" When i am running, i feel better than before. Too much studying just not me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sampleText)\n",
    "print(\"\\nStemming Results:\")\n",
    "for word in words:\n",
    "    print(f\"{word} --> {stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLemmatization Results:\")\n",
    "for word in words:\n",
    "    print(f\"{word} --> {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that show difference between stemming and lemmatization\n",
    "print(\"\\nStrong Examples Showing Differences:\")\n",
    "sample_words = [\"running\", \"flies\", \"better\", \"flying\", \"studies\", \"feet\", \"ate\"]\n",
    "for word in sample_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word} --> Stem: {stemmed}, Lemma: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí°üí° Brainy Bites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpStemmer\n",
    "\n",
    "RegexpStemmer is a rule-based stemmer in NLTK that uses regular expressions to strip word suffixes. Unlike PorterStemmer or LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying RegexpStemmer\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "R_stem=RegexpStemmer(\"ing$|s$|e$|able$\",min=4)\n",
    "arg=['eats','geting',\"disable\"]\n",
    "for i in arg:\n",
    "    print(R_stem.stem(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SnowBallStemmer\n",
    "It‚Äôs also called ‚ÄúPorter2‚Äù stemmer.\n",
    "More advanced than PorterStemmer with better linguistic rules.\n",
    "Supports multiple languages, unlike PorterStemmer (which only supports English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "Snowballstem=SnowballStemmer(\"english\")\n",
    "for  i in arg:\n",
    "    print(i+\".....\"+Snowballstem.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still Confused??ü•≤\n",
    "### Here you go üòé Full Comparison Table:\n",
    " | Method              | Key Feature                                                 | Returns Real Word? | Based on Rules or Dictionary? | Accuracy       | Language Support      |\n",
    " |---------------------|-------------------------------------------------------------|---------------------|-------------------------------|----------------|------------------------|\n",
    " | Porter Stemmer      | Oldest and simplest stemmer, chops suffixes                | ‚ùå (Not always)     | Rule-based                    | Low to Medium  | English only           |\n",
    " | Lemmatization       | Uses vocabulary + grammar (POS tagging) to get base word   | ‚úÖ (Always)         | Dictionary-based              | High           | English (WordNet)      |\n",
    " | RegexpStemmer       | Applies custom regex rules (e.g., remove -ing, -ed, -s)    | ‚ùå (Usually not)    | Rule-based (regex)            | Low            | Depends on rules       |\n",
    "| Snowball Stemmer    | More advanced than Porter, improved linguistic logic       | ‚ùå (Not always)     | Rule-based (refined)          | Medium to High | Multiple languages     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Stop Words\n",
    "Stop words are common words that usually carry less meaning and are often removed from text data during preprocessing.\n",
    "Examples include: \"is\", \"and\", \"the\", \"a\", \"in\", etc.\n",
    "Removing them helps in focusing on more meaningful words in NLP tasks.\n",
    "\n",
    "**How it removing them helps?**\n",
    "- They occur frequently but add little information.\n",
    "- Reducing them simplifies the dataset and improves model performance.\n",
    "- Eliminating them reduces noise in the data.\n",
    "- It helps in decreasing the size of the dataset.\n",
    "- Enhances the efficiency of text classification and search-related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "words = word_tokenize(sampleText)\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english')) # setting the language\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"\\nOriginal Words:\")\n",
    "print(words)\n",
    "\n",
    "print(\"\\nFiltered Words (Stop Words Removed):\")\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is NER?**\n",
    "\n",
    "Named Entity Recognition (NER) is the process of locating and classifying named entities in text into predefined categories such as:\n",
    "- Person names\n",
    "- Organizations\n",
    "- Locations\n",
    "- Dates\n",
    "- Time expressions\n",
    "- Monetary values\n",
    "\n",
    "NER helps in understanding the meaning of text by identifying important nouns that have a specific meaning or role.\n",
    "\n",
    "### Why is NER important?\n",
    "- Helps extract structured information from unstructured text.\n",
    "- Useful in applications like chatbots, search engines, recommendation systems, and summarization.\n",
    "- Makes information retrieval more accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple was founded by Steve Jobs in Cupertino in 1976.\"\n",
    "# Try your own!\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"-\", ent.label_)\n",
    "\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "Apple - ORG\n",
    "Steve Jobs - PERSON\n",
    "Cupertino - GPE\n",
    "1976 - DATE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Test Your Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "# ü•≥ü•≥ Congrats Everyone we did it!!\n",
    "Now you know a lot more than before you.\n",
    "### You should pat your back. Coz i certainly am.\n",
    "\n",
    "Thankyou for being a part of it. \n",
    "Happy Creative Coding!!! \n",
    "# ü§òü§ò\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
